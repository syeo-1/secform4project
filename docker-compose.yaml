version: '3'

services:
  airflow-initialization:
    image: apache/airflow:2.7.2
    entrypoint: ["/bin/sh", "-c", "airflow db upgrade && cp -r /tmp/dags/. /opt/airflow/dags"]
    volumes:
      - ./dags:/tmp/dags # mount local dags to /tmp/dags
      - airflow-dags:/opt/airflow/dags
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${HOST}/${DB_NAME}
      # - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    command: /bin/bash  # Prevent it from running something else after entrypoint
  
  airflow-webserver:
    image: apache/airflow:2.7.2
    restart: always
    depends_on:
      - airflow-initialization
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor # allows for parallelism for eventual prod
      - AIRFLOW__CORE__LOAD_EXAMPLES=False # don't display example dags provided by airflow by default
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${HOST}/${DB_NAME}
      - AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.www.auth.backend.default
    volumes:
      - airflow-dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - airflow-plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.7.2
    restart: always
    depends_on:
      - airflow-initialization # ensure airflow initialization is done before the scheduler comes up
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${HOST}/${DB_NAME}
    volumes:
      - ./dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
    command: scheduler # runs the dags

volumes: # ensure the following volumes are all managed by docker, not manual/by the system
  airflow-dags: # for dags
  airflow-logs: # for logs
  airflow-plugins: # for plugins
